# encoding:utf-8
"""
LLM Bot æ¨¡å—
å¯¹æ¥ç§æœ‰åŒ–LLMç½‘å…³ï¼ˆOpenAIæ ‡å‡†æ¥å£ï¼‰
"""

import time
from typing import Dict, List, Optional

import openai
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from common.logger import logger
from common.session_manager import get_session_manager
from config import get_config


class LLMBot:
    """
    LLMæœºå™¨äºº
    æ”¯æŒOpenAIæ ‡å‡†æ¥å£çš„ç§æœ‰åŒ–LLMç½‘å…³
    """
    
    def __init__(self):
        self.config = get_config()
        self._session: Optional[requests.Session] = None
        self._setup_openai()
        logger.info("[LLMBot] Initialized")
    
    def _get_session(self) -> requests.Session:
        """è·å–æˆ–åˆ›å»ºå¸¦è¿æ¥æ± çš„ Session"""
        if self._session is None:
            self._session = requests.Session()
            
            # é…ç½®é‡è¯•ç­–ç•¥ï¼šè¿æ¥é”™è¯¯æ—¶è‡ªåŠ¨é‡è¯•
            retry_strategy = Retry(
                total=3,
                backoff_factor=0.5,
                status_forcelist=[429, 500, 502, 503, 504],
                allowed_methods=["HEAD", "GET", "OPTIONS", "POST"]
            )
            
            # é…ç½®è¿æ¥æ± ï¼šä¿æŒè¿æ¥æ´»è·ƒï¼Œé˜²æ­¢è¢«ç½‘å…³åˆ‡æ–­
            adapter = HTTPAdapter(
                pool_connections=10,
                pool_maxsize=20,
                max_retries=retry_strategy,
                pool_block=False
            )
            
            self._session.mount("http://", adapter)
            self._session.mount("https://", adapter)
            
            # è®¾ç½® keep-alive header
            self._session.headers.update({
                "Connection": "keep-alive",
                "Keep-Alive": "timeout=60, max=1000"
            })
            
            # å°† session è®¾ç½®ç»™ openai
            openai.requestssession = self._session
            
        return self._session
    
    def _setup_openai(self):
        """é…ç½®OpenAIå®¢æˆ·ç«¯"""
        # è®¾ç½®API Key
        openai.api_key = self.config.get("llm_api_key", "")
        
        # è®¾ç½®API Baseï¼ˆç§æœ‰åŒ–ç½‘å…³åœ°å€ï¼‰
        api_base = self.config.get("llm_api_base", "")
        if api_base:
            openai.api_base = api_base
            logger.info(f"[LLMBot] Using custom API base: {api_base}")
        
        # åˆå§‹åŒ– sessionï¼ˆè¿æ¥æ± ï¼‰
        self._get_session()
    
    def chat(self, query: str, session_id: str, context: Optional[Dict] = None) -> str:
        """
        å¯¹è¯
        
        :param query: ç”¨æˆ·è¾“å…¥
        :param session_id: ä¼šè¯ID
        :param context: é¢å¤–ä¸Šä¸‹æ–‡
        :return: æœºå™¨äººå›å¤
        """
        try:
            # å¤„ç†ç‰¹æ®Šå‘½ä»¤
            reply = self._handle_command(query, session_id)
            if reply:
                return reply
            
            # è·å–ä¼šè¯å¹¶æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            session_manager = get_session_manager()
            session = session_manager.get_session(session_id)
            
            # å¦‚æœæ˜¯æ–°ä¼šè¯ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º
            if not session.messages:
                character_desc = self.config.get("character_desc", "")
                if character_desc:
                    session.add_message("system", character_desc)
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            session.add_message("user", query)
            
            # è°ƒç”¨LLMï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            response = self._call_llm_with_retry(session.get_messages())
            
            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°ä¼šè¯
            session.add_message("assistant", response)
            
            return response
            
        except Exception as e:
            logger.error(f"[LLMBot] Chat error: {e}")
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def _handle_command(self, query: str, session_id: str) -> Optional[str]:
        """
        å¤„ç†å†…ç½®å‘½ä»¤
        
        :param query: ç”¨æˆ·è¾“å…¥
        :param session_id: ä¼šè¯ID
        :return: å¦‚æœæ˜¯å‘½ä»¤è¿”å›å›å¤ï¼Œå¦åˆ™è¿”å›None
        """
        query = query.strip()
        
        # æ¸…é™¤è®°å¿†å‘½ä»¤
        clear_commands = ["#æ¸…é™¤è®°å¿†", "#æ¸…ç©º", "#reset", "/reset", "/clear"]
        if query in clear_commands:
            session_manager = get_session_manager()
            session_manager.clear_session(session_id)
            return "ğŸ§¹ ä¼šè¯è®°å¿†å·²æ¸…é™¤ï¼Œè®©æˆ‘ä»¬å¼€å§‹æ–°çš„å¯¹è¯å§ï¼"
        
        # å¸®åŠ©å‘½ä»¤
        help_commands = ["#å¸®åŠ©", "#help", "/help"]
        if query in help_commands:
            return self._get_help_text()
        
        return None
    
    def _get_help_text(self) -> str:
        """è·å–å¸®åŠ©æ–‡æœ¬"""
        return """ğŸ¤– **WPSæ™ºèƒ½åŠ©æ‰‹ä½¿ç”¨æŒ‡å—**

**åŸºæœ¬åŠŸèƒ½ï¼š**
â€¢ ç›´æ¥å‘é€æ¶ˆæ¯ä¸æˆ‘å¯¹è¯
â€¢ æˆ‘å¯ä»¥å›ç­”é—®é¢˜ã€ç”Ÿæˆå†…å®¹ã€è¾…åŠ©åŠå…¬

**å¸¸ç”¨å‘½ä»¤ï¼š**
â€¢ `#å¸®åŠ©` / `#help` - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
â€¢ `#æ¸…é™¤è®°å¿†` / `#reset` - æ¸…ç©ºå½“å‰ä¼šè¯è®°å¿†

**æç¤ºï¼š**
â€¢ åœ¨ç¾¤èŠä¸­@æˆ‘å³å¯è§¦å‘å¯¹è¯
â€¢ ç§èŠå¯ç›´æ¥å‘é€æ¶ˆæ¯
â€¢ æˆ‘ä¼šè®°ä½å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œæ–¹ä¾¿è¿ç»­äº¤æµ"""
    
    def _call_llm(self, messages: List[Dict]) -> str:
        """
        è°ƒç”¨LLM API
        
        :param messages: æ¶ˆæ¯åˆ—è¡¨
        :return: LLMå›å¤
        """
        try:
            model = self.config.get("llm_model", "gpt-3.5-turbo")
            temperature = self.config.get("temperature", 0.7)
            max_tokens = self.config.get("max_tokens", 2048)
            request_timeout = self.config.get("request_timeout", 120)
            
            logger.debug(f"[LLMBot] Calling LLM with {len(messages)} messages")
            
            # ç¡®ä¿ä½¿ç”¨å¸¦è¿æ¥æ± çš„ session
            self._get_session()
            
            response = openai.ChatCompletion.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=self.config.get("top_p", 1.0),
                frequency_penalty=self.config.get("frequency_penalty", 0.0),
                presence_penalty=self.config.get("presence_penalty", 0.0),
                request_timeout=request_timeout
            )
            
            content = response.choices[0].message.content
            usage = response.get("usage", {})
            
            logger.info(
                f"[LLMBot] LLM response received, "
                f"tokens: prompt={usage.get('prompt_tokens', 0)}, "
                f"completion={usage.get('completion_tokens', 0)}"
            )
            
            return content.strip()
            
        except openai.error.RateLimitError as e:
            logger.error(f"[LLMBot] Rate limit error: {e}")
            return "âš ï¸ è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•"
        
        except openai.error.Timeout as e:
            logger.error(f"[LLMBot] Timeout error: {e}")
            return "â±ï¸ è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•"
        
        except openai.error.APIError as e:
            logger.error(f"[LLMBot] API error: {e}")
            return f"ğŸ”Œ APIé”™è¯¯ï¼š{str(e)}"
        
        except (ConnectionError, requests.exceptions.ConnectionError) as e:
            # æ•è·è¿æ¥é‡ç½®é”™è¯¯ï¼Œäº¤ç”±ä¸Šå±‚é‡è¯•
            logger.warning(f"[LLMBot] Connection error (will retry): {e}")
            raise
        
        except Exception as e:
            error_msg = str(e).lower()
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥é‡ç½®ç›¸å…³é”™è¯¯
            if any(kw in error_msg for kw in ["connection", "reset", "aborted", "peer", "broken pipe"]):
                logger.warning(f"[LLMBot] Connection reset detected (will retry): {e}")
                raise ConnectionError(f"Connection reset: {e}")
            logger.error(f"[LLMBot] LLM call failed: {e}")
            raise
    
    def _call_llm_with_retry(self, messages: List[Dict], retry_count: int = 0) -> str:
        """
        å¸¦é‡è¯•çš„LLMè°ƒç”¨
        
        :param messages: æ¶ˆæ¯åˆ—è¡¨
        :param retry_count: å½“å‰é‡è¯•æ¬¡æ•°
        :return: LLMå›å¤
        """
        max_retries = 2
        
        try:
            return self._call_llm(messages)
        except Exception as e:
            if retry_count < max_retries:
                wait_time = 3 * (retry_count + 1)
                error_type = type(e).__name__
                logger.warning(f"[LLMBot] {error_type} - Retry {retry_count + 1}/{max_retries} after {wait_time}s: {e}")
                time.sleep(wait_time)
                
                # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œé‡ç½® session ä»¥åˆ›å»ºæ–°è¿æ¥
                if isinstance(e, (ConnectionError, requests.exceptions.ConnectionError)) or \
                   any(kw in str(e).lower() for kw in ["connection", "reset", "aborted"]):
                    logger.info("[LLMBot] Resetting connection pool for retry")
                    self._reset_session()
                
                return self._call_llm_with_retry(messages, retry_count + 1)
            else:
                raise
    
    def _reset_session(self):
        """é‡ç½®è¿æ¥æ± ï¼Œç”¨äºè¿æ¥é”™è¯¯åé‡å»ºè¿æ¥"""
        if self._session:
            try:
                self._session.close()
            except Exception:
                pass
        self._session = None
        openai.requestssession = None
        # é‡æ–°åˆå§‹åŒ–
        self._get_session()


# å…¨å±€Botå®ä¾‹
_bot: Optional[LLMBot] = None


def get_bot() -> LLMBot:
    """è·å–å…¨å±€Botå®ä¾‹"""
    global _bot
    if _bot is None:
        _bot = LLMBot()
    return _bot
